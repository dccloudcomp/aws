{"filter":false,"title":"main.py","tooltip":"/terraform/lambda_package/main.py","undoManager":{"mark":1,"position":1,"stack":[[{"start":{"row":0,"column":0},"end":{"row":164,"column":0},"action":"insert","lines":["import os","import io","import json","import csv","import time","import uuid","import logging","from typing import List, Dict","","import boto3","import requests","import pandas as pd","","logger = logging.getLogger()","logger.setLevel(logging.INFO)","","s3 = boto3.client(\"s3\")","","HF_API_TOKEN = os.environ.get(\"HF_API_TOKEN\")","HF_MODEL_ID = os.environ.get(\"HF_MODEL_ID\", \"cardiffnlp/twitter-roberta-base-sentiment-latest\")","INGEST_BUCKET = os.environ[\"INGEST_BUCKET\"]","WEBSITE_BUCKET = os.environ[\"WEBSITE_BUCKET\"]","RESULTS_PREFIX = os.environ.get(\"RESULTS_PREFIX\", \"results\")","CSV_SUFFIX = os.environ.get(\"CSV_SUFFIX\", \".csv\")","","HF_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL_ID}\"","HF_HEADERS = {\"Authorization\": f\"Bearer {HF_API_TOKEN}\"} if HF_API_TOKEN else {}","","# Ajusta si tu CSV tiene otras columnas","TEXT_COLUMNS_CANDIDATES = [\"text\", \"tweet\", \"content\", \"message\", \"body\"]","","def call_hf(texts: List[str]) -> List[Dict]:","    \"\"\"","    Llama a la Inference API por lotes, respetando throttling.","    \"\"\"","    outputs = []","    for t in texts:","        payload = {\"inputs\": t}","        # Reintentos simples para 503/429","        for attempt in range(4):","            resp = requests.post(HF_URL, headers=HF_HEADERS, json=payload, timeout=30)","            if resp.status_code == 200:","                outputs.append(resp.json())","                break","            elif resp.status_code in (429, 503):","                wait = 1.5 * (attempt + 1)","                logger.warning(f\"HF throttled ({resp.status_code}). Retrying in {wait}s...\")","                time.sleep(wait)","                continue","            else:","                logger.error(f\"HF error {resp.status_code}: {resp.text[:200]}\")","                outputs.append({\"error\": resp.text})","                break","        else:","            outputs.append({\"error\": \"Exceeded retries\"})","    return outputs","","def extract_sentiment(hf_output) -> Dict[str, float | str]:","    \"\"\"","    Normaliza salida de modelos de sentimiento típicos (e.g., cardiffnlp).","    \"\"\"","    try:","        # cardiffnlp devuelve lista de [{label, score}...]","        preds = hf_output[0] if isinstance(hf_output, list) and hf_output and isinstance(hf_output[0], list) else hf_output","        if isinstance(preds, list) and preds and isinstance(preds[0], dict) and \"label\" in preds[0]:","            # devolver la etiqueta con mayor score + mapa de probabilidades","            best = max(preds, key=lambda x: x[\"score\"])","            probs = {p[\"label\"]: float(p[\"score\"]) for p in preds}","            return {\"label\": best[\"label\"], **probs}","        # Otros formatos","        return {\"label\": \"UNKNOWN\"}","    except Exception as e:","        logger.exception(\"Failed to parse HF output\")","        return {\"label\": \"ERROR\", \"error\": str(e)}","","def read_csv_from_s3(bucket: str, key: str) -> pd.DataFrame:","    obj = s3.get_object(Bucket=bucket, Key=key)","    body = obj[\"Body\"].read()","    # Intentar con pandas (maneja comillas y separadores)","    df = pd.read_csv(io.BytesIO(body))","    # Detectar columna de texto","    text_col = None","    lower_cols = {c.lower(): c for c in df.columns}","    for cand in TEXT_COLUMNS_CANDIDATES:","        if cand in lower_cols:","            text_col = lower_cols[cand]","            break","    if text_col is None:","        # Si no se reconoce, intenta primera columna con strings","        for c in df.columns:","            if df[c].dtype == object:","                text_col = c","                break","    if text_col is None:","        raise ValueError(\"No se encontró columna de texto. Renombra una columna a 'text' o 'tweet'.\")","    return df, text_col","","def save_results(upload_id: str, records: List[Dict]):","    # Guardar JSONL y resumen en website bucket","    result_dir = f\"{RESULTS_PREFIX}/{upload_id}\"","    # JSONL por filas","    jsonl_key = f\"{result_dir}/predictions.jsonl\"","    summary_key = f\"{result_dir}/summary.json\"","","    jsonl_bytes = \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in records).encode(\"utf-8\")","    s3.put_object(Bucket=WEBSITE_BUCKET, Key=jsonl_key, Body=jsonl_bytes, ContentType=\"application/json\")","","    # Resumen simple","    counts = {}","    for r in records:","        lbl = r.get(\"sentiment\", \"UNKNOWN\")","        counts[lbl] = counts.get(lbl, 0) + 1","","    summary = {","        \"upload_id\": upload_id,","        \"total\": len(records),","        \"counts\": counts,","        \"jsonl\": jsonl_key,","        \"generated_at\": int(time.time())","    }","    s3.put_object(","        Bucket=WEBSITE_BUCKET,","        Key=summary_key,","        Body=json.dumps(summary, ensure_ascii=False).encode(\"utf-8\"),","        ContentType=\"application/json\",","    )","","def handler(event, context):","    logger.info(\"Event: %s\", json.dumps(event))","    # S3 trigger","    for rec in event.get(\"Records\", []):","        s3info = rec.get(\"s3\", {})","        b = s3info.get(\"bucket\", {}).get(\"name\")","        k = s3info.get(\"object\", {}).get(\"key\")","        if not k.endswith(CSV_SUFFIX):","            logger.info(f\"Ignoring non-CSV key: {k}\")","            continue","","        upload_id = uuid.uuid4().hex[:8]","        logger.info(f\"Processing s3://{b}/{k} upload_id={upload_id}\")","","        df, text_col = read_csv_from_s3(b, k)","        texts = df[text_col].astype(str).fillna(\"\").tolist()","","        results = call_hf(texts)","        normalized = [extract_sentiment(o) for o in results]","","        # Ensamblar registros finales","        records = []","        for i, row in enumerate(df.itertuples(index=False)):","            base = {c: getattr(row, c) for c in df.columns}","            sent = normalized[i]","            label = sent.get(\"label\", \"UNKNOWN\")","            base[\"sentiment\"] = label","            # Añade scores si existen","            for k2, v2 in sent.items():","                if k2 != \"label\":","                    base[f\"score_{k2}\"] = v2","            records.append(base)","","        save_results(upload_id, records)","        logger.info(f\"Saved results to s3://{WEBSITE_BUCKET}/{RESULTS_PREFIX}/{upload_id}/\")","","    return {\"status\": \"ok\"}",""],"id":1}],[{"start":{"row":0,"column":0},"end":{"row":164,"column":0},"action":"remove","lines":["import os","import io","import json","import csv","import time","import uuid","import logging","from typing import List, Dict","","import boto3","import requests","import pandas as pd","","logger = logging.getLogger()","logger.setLevel(logging.INFO)","","s3 = boto3.client(\"s3\")","","HF_API_TOKEN = os.environ.get(\"HF_API_TOKEN\")","HF_MODEL_ID = os.environ.get(\"HF_MODEL_ID\", \"cardiffnlp/twitter-roberta-base-sentiment-latest\")","INGEST_BUCKET = os.environ[\"INGEST_BUCKET\"]","WEBSITE_BUCKET = os.environ[\"WEBSITE_BUCKET\"]","RESULTS_PREFIX = os.environ.get(\"RESULTS_PREFIX\", \"results\")","CSV_SUFFIX = os.environ.get(\"CSV_SUFFIX\", \".csv\")","","HF_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL_ID}\"","HF_HEADERS = {\"Authorization\": f\"Bearer {HF_API_TOKEN}\"} if HF_API_TOKEN else {}","","# Ajusta si tu CSV tiene otras columnas","TEXT_COLUMNS_CANDIDATES = [\"text\", \"tweet\", \"content\", \"message\", \"body\"]","","def call_hf(texts: List[str]) -> List[Dict]:","    \"\"\"","    Llama a la Inference API por lotes, respetando throttling.","    \"\"\"","    outputs = []","    for t in texts:","        payload = {\"inputs\": t}","        # Reintentos simples para 503/429","        for attempt in range(4):","            resp = requests.post(HF_URL, headers=HF_HEADERS, json=payload, timeout=30)","            if resp.status_code == 200:","                outputs.append(resp.json())","                break","            elif resp.status_code in (429, 503):","                wait = 1.5 * (attempt + 1)","                logger.warning(f\"HF throttled ({resp.status_code}). Retrying in {wait}s...\")","                time.sleep(wait)","                continue","            else:","                logger.error(f\"HF error {resp.status_code}: {resp.text[:200]}\")","                outputs.append({\"error\": resp.text})","                break","        else:","            outputs.append({\"error\": \"Exceeded retries\"})","    return outputs","","def extract_sentiment(hf_output) -> Dict[str, float | str]:","    \"\"\"","    Normaliza salida de modelos de sentimiento típicos (e.g., cardiffnlp).","    \"\"\"","    try:","        # cardiffnlp devuelve lista de [{label, score}...]","        preds = hf_output[0] if isinstance(hf_output, list) and hf_output and isinstance(hf_output[0], list) else hf_output","        if isinstance(preds, list) and preds and isinstance(preds[0], dict) and \"label\" in preds[0]:","            # devolver la etiqueta con mayor score + mapa de probabilidades","            best = max(preds, key=lambda x: x[\"score\"])","            probs = {p[\"label\"]: float(p[\"score\"]) for p in preds}","            return {\"label\": best[\"label\"], **probs}","        # Otros formatos","        return {\"label\": \"UNKNOWN\"}","    except Exception as e:","        logger.exception(\"Failed to parse HF output\")","        return {\"label\": \"ERROR\", \"error\": str(e)}","","def read_csv_from_s3(bucket: str, key: str) -> pd.DataFrame:","    obj = s3.get_object(Bucket=bucket, Key=key)","    body = obj[\"Body\"].read()","    # Intentar con pandas (maneja comillas y separadores)","    df = pd.read_csv(io.BytesIO(body))","    # Detectar columna de texto","    text_col = None","    lower_cols = {c.lower(): c for c in df.columns}","    for cand in TEXT_COLUMNS_CANDIDATES:","        if cand in lower_cols:","            text_col = lower_cols[cand]","            break","    if text_col is None:","        # Si no se reconoce, intenta primera columna con strings","        for c in df.columns:","            if df[c].dtype == object:","                text_col = c","                break","    if text_col is None:","        raise ValueError(\"No se encontró columna de texto. Renombra una columna a 'text' o 'tweet'.\")","    return df, text_col","","def save_results(upload_id: str, records: List[Dict]):","    # Guardar JSONL y resumen en website bucket","    result_dir = f\"{RESULTS_PREFIX}/{upload_id}\"","    # JSONL por filas","    jsonl_key = f\"{result_dir}/predictions.jsonl\"","    summary_key = f\"{result_dir}/summary.json\"","","    jsonl_bytes = \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in records).encode(\"utf-8\")","    s3.put_object(Bucket=WEBSITE_BUCKET, Key=jsonl_key, Body=jsonl_bytes, ContentType=\"application/json\")","","    # Resumen simple","    counts = {}","    for r in records:","        lbl = r.get(\"sentiment\", \"UNKNOWN\")","        counts[lbl] = counts.get(lbl, 0) + 1","","    summary = {","        \"upload_id\": upload_id,","        \"total\": len(records),","        \"counts\": counts,","        \"jsonl\": jsonl_key,","        \"generated_at\": int(time.time())","    }","    s3.put_object(","        Bucket=WEBSITE_BUCKET,","        Key=summary_key,","        Body=json.dumps(summary, ensure_ascii=False).encode(\"utf-8\"),","        ContentType=\"application/json\",","    )","","def handler(event, context):","    logger.info(\"Event: %s\", json.dumps(event))","    # S3 trigger","    for rec in event.get(\"Records\", []):","        s3info = rec.get(\"s3\", {})","        b = s3info.get(\"bucket\", {}).get(\"name\")","        k = s3info.get(\"object\", {}).get(\"key\")","        if not k.endswith(CSV_SUFFIX):","            logger.info(f\"Ignoring non-CSV key: {k}\")","            continue","","        upload_id = uuid.uuid4().hex[:8]","        logger.info(f\"Processing s3://{b}/{k} upload_id={upload_id}\")","","        df, text_col = read_csv_from_s3(b, k)","        texts = df[text_col].astype(str).fillna(\"\").tolist()","","        results = call_hf(texts)","        normalized = [extract_sentiment(o) for o in results]","","        # Ensamblar registros finales","        records = []","        for i, row in enumerate(df.itertuples(index=False)):","            base = {c: getattr(row, c) for c in df.columns}","            sent = normalized[i]","            label = sent.get(\"label\", \"UNKNOWN\")","            base[\"sentiment\"] = label","            # Añade scores si existen","            for k2, v2 in sent.items():","                if k2 != \"label\":","                    base[f\"score_{k2}\"] = v2","            records.append(base)","","        save_results(upload_id, records)","        logger.info(f\"Saved results to s3://{WEBSITE_BUCKET}/{RESULTS_PREFIX}/{upload_id}/\")","","    return {\"status\": \"ok\"}",""],"id":2},{"start":{"row":0,"column":0},"end":{"row":113,"column":0},"action":"insert","lines":["import os, io, json, time, uuid, logging, csv","from typing import List, Dict","import boto3, requests","","logger = logging.getLogger()","logger.setLevel(logging.INFO)","","s3 = boto3.client(\"s3\")","","HF_API_TOKEN = os.environ.get(\"HF_API_TOKEN\")","HF_MODEL_ID = os.environ.get(\"HF_MODEL_ID\", \"cardiffnlp/twitter-roberta-base-sentiment-latest\")","INGEST_BUCKET = os.environ[\"INGEST_BUCKET\"]","WEBSITE_BUCKET = os.environ[\"WEBSITE_BUCKET\"]","RESULTS_PREFIX = os.environ.get(\"RESULTS_PREFIX\", \"results\")","CSV_SUFFIX = os.environ.get(\"CSV_SUFFIX\", \".csv\")","","HF_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL_ID}\"","HF_HEADERS = {\"Authorization\": f\"Bearer {HF_API_TOKEN}\"} if HF_API_TOKEN else {}","","TEXT_COLUMNS_CANDIDATES = [\"text\", \"tweet\", \"content\", \"message\", \"body\"]","","def call_hf(text: str):","    payload = {\"inputs\": text}","    for attempt in range(4):","        resp = requests.post(HF_URL, headers=HF_HEADERS, json=payload, timeout=30)","        if resp.status_code == 200:","            return resp.json()","        if resp.status_code in (429, 503):","            wait = 1.5 * (attempt + 1)","            time.sleep(wait)","            continue","    return {\"error\": f\"HF status {resp.status_code} -> {resp.text[:200]}\"}","","def extract_sentiment(hf_output) -> Dict[str, float | str]:","    try:","        preds = hf_output[0] if isinstance(hf_output, list) and hf_output and isinstance(hf_output[0], list) else hf_output","        if isinstance(preds, list) and preds and isinstance(preds[0], dict) and \"label\" in preds[0]:","            best = max(preds, key=lambda x: x[\"score\"])","            probs = {p[\"label\"]: float(p[\"score\"]) for p in preds}","            return {\"label\": best[\"label\"], **probs}","        return {\"label\": \"UNKNOWN\"}","    except Exception as e:","        return {\"label\": \"ERROR\", \"error\": str(e)}","","def detect_text_column(header: List[str]) -> str | None:","    lower = {c.lower(): c for c in header}","    for cand in TEXT_COLUMNS_CANDIDATES:","        if cand in lower:","            return lower[cand]","    return header[0] if header else None","","def process_csv_streaming(bucket: str, key: str):","    resp = s3.get_object(Bucket=bucket, Key=key)","    body = resp[\"Body\"].read()","    text_stream = io.StringIO(body.decode(\"utf-8\", errors=\"ignore\"))","    reader = csv.DictReader(text_stream)","    header = reader.fieldnames or []","    text_col = detect_text_column(header)","    if not text_col:","        raise ValueError(\"No se encontró columna de texto en el CSV.\")","","    records = []","    for row in reader:","        txt = (row.get(text_col) or \"\").strip()","        out = call_hf(txt) if txt else {\"label\": \"EMPTY\"}","        norm = extract_sentiment(out)","        row[\"sentiment\"] = norm.get(\"label\", \"UNKNOWN\")","        for k, v in norm.items():","            if k != \"label\":","                row[f\"score_{k}\"] = v","        records.append(row)","    return records","","def save_results(upload_id: str, records: List[Dict]):","    result_dir = f\"{RESULTS_PREFIX}/{upload_id}\"","    jsonl_key = f\"{result_dir}/predictions.jsonl\"","    summary_key = f\"{result_dir}/summary.json\"","","    jsonl_bytes = \"\\n\".join(json.dumps(r, ensure_ascii=False) for r in records).encode(\"utf-8\")","    s3.put_object(Bucket=WEBSITE_BUCKET, Key=jsonl_key, Body=jsonl_bytes, ContentType=\"application/json\")","","    counts = {}","    for r in records:","        lbl = r.get(\"sentiment\", \"UNKNOWN\")","        counts[lbl] = counts.get(lbl, 0) + 1","","    summary = {","        \"upload_id\": upload_id,","        \"total\": len(records),","        \"counts\": counts,","        \"jsonl\": jsonl_key,","        \"generated_at\": int(time.time())","    }","    s3.put_object(","        Bucket=WEBSITE_BUCKET,","        Key=summary_key,","        Body=json.dumps(summary, ensure_ascii=False).encode(\"utf-8\"),","        ContentType=\"application/json\"","    )","","def handler(event, context):","    logger.info(\"Event: %s\", json.dumps(event))","    for rec in event.get(\"Records\", []):","        s3info = rec.get(\"s3\", {})","        b = s3info.get(\"bucket\", {}).get(\"name\")","        k = s3info.get(\"object\", {}).get(\"key\")","        if not k.endswith(CSV_SUFFIX):","            continue","        upload_id = uuid.uuid4().hex[:8]","        records = process_csv_streaming(b, k)","        save_results(upload_id, records)","        logger.info(f\"Saved results to s3://{WEBSITE_BUCKET}/{RESULTS_PREFIX}/{upload_id}/\")","    return {\"status\": \"ok\"}",""]}]]},"ace":{"folds":[],"scrolltop":1521.5,"scrollleft":0,"selection":{"start":{"row":113,"column":0},"end":{"row":113,"column":0},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":107,"state":"start","mode":"ace/mode/python"}},"timestamp":1754669135474,"hash":"a67f8d5b6a73d91c307a7d615bae1f51e53d6023"}